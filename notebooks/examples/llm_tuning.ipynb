{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Tuning with LionAGI's RL System\n",
    "\n",
    "This notebook demonstrates how to use LionAGI's RL capabilities to improve LLM performance through:\n",
    "1. Parameter tuning\n",
    "2. Synthetic data generation\n",
    "3. Hybrid optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from lionagi import Branch, iModel\n",
    "from lionagi.operations.rl import OptimizationMode, TuningConfig, RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Training Data\n",
    "\n",
    "Let's create some example training data for a task (e.g., improving JSON generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training data\n",
    "training_data = [\n",
    "    {\n",
    "        \"prompt\": \"Convert this to JSON: name: John Smith, age: 30, city: New York\",\n",
    "        \"target\": '{\"name\": \"John Smith\", \"age\": 30, \"city\": \"New York\"}',\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Create JSON for: title: Software Engineer, company: TechCo, years: 5\",\n",
    "        \"target\": '{\"title\": \"Software Engineer\", \"company\": \"TechCo\", \"years\": 5}',\n",
    "    },\n",
    "]\n",
    "\n",
    "# Evaluation data\n",
    "eval_data = [\n",
    "    {\n",
    "        \"prompt\": \"Make JSON from: product: Laptop, price: 999.99, brand: TechBrand\",\n",
    "        \"target\": '{\"product\": \"Laptop\", \"price\": 999.99, \"brand\": \"TechBrand\"}',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Branch and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = iModel(provider=\"openai\", model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Create branch with system prompt\n",
    "branch = Branch(\n",
    "    name=\"JsonGenerator\",\n",
    "    system=\"You are an expert at converting text to valid JSON format.\",\n",
    "    chat_model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Reward Function\n",
    "\n",
    "Let's define a reward function that checks JSON validity and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "async def json_reward(response: str, target: str) -> float:\n",
    "    \"\"\"Compute reward based on JSON validity and similarity\"\"\"\n",
    "    try:\n",
    "        # Check JSON validity\n",
    "        json.loads(response)\n",
    "\n",
    "        # Compute string similarity\n",
    "        similarity = SequenceMatcher(None, response, target).ratio()\n",
    "\n",
    "        # Return combined score\n",
    "        return 0.5 + (\n",
    "            0.5 * similarity\n",
    "        )  # 0.5 for valid JSON, up to 0.5 for similarity\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        return 0.0  # Invalid JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    \"max_steps\": 100,\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"target_reward\": 0.95,\n",
    "    \"eval_interval\": 10,\n",
    "}\n",
    "\n",
    "# Run training with synthetic data generation and parameter tuning\n",
    "results = await RL(\n",
    "    branch=branch,\n",
    "    training_data=training_data,\n",
    "    eval_data=eval_data,\n",
    "    mode=OptimizationMode.HYBRID,\n",
    "    tuning_config=config,\n",
    "    reward_fn=json_reward,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract evaluation metrics\n",
    "eval_rewards = [r[\"metrics\"][\"reward\"] for r in results[\"eval_results\"]]\n",
    "eval_steps = [i * config[\"eval_interval\"] for i in range(len(eval_rewards))]\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(eval_steps, eval_rewards, marker=\"o\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Evaluation Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new example\n",
    "test_prompt = (\n",
    "    \"Create JSON for: user: Alice Cooper, email: alice@email.com, active: true\"\n",
    ")\n",
    "\n",
    "response = await branch.communicate(test_prompt)\n",
    "print(\"Generated JSON:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
