{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with LionAGI\n",
    "\n",
    "This notebook demonstrates how to use the reinforcement learning functionality in lionagi with a DQN agent learning in a GridWorld environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lionagi import Branch\n",
    "from lionagi.operations.rl.implementations.dqn import DQNAgent\n",
    "from lionagi.operations.rl.implementations.gridworld import GridWorldEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "\n",
    "First, let's create a GridWorld environment instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(\n",
    "    size=(8, 8), max_steps=100, random_obstacles=True, n_obstacles=10\n",
    ")\n",
    "\n",
    "# Let's look at the initial grid\n",
    "state = await env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "\n",
    "Now we'll create a DQN agent to learn in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    name=\"DQNGridWorld\",\n",
    "    state_space=env.state_space,\n",
    "    action_space=env.action_space,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=0.001,\n",
    "    memory_capacity=10000,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Branch and Train\n",
    "\n",
    "Now we'll create a Branch to manage the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = Branch(name=\"RLTraining\")\n",
    "\n",
    "# Train the agent\n",
    "metrics = await branch.operate_rl(\n",
    "    agent=agent,\n",
    "    environment=env,\n",
    "    max_episodes=1000,\n",
    "    max_steps_per_episode=100,\n",
    "    target_reward=0.95,  # Stop when average reward exceeds this\n",
    "    log_interval=10,\n",
    "    track_metrics=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Progress\n",
    "\n",
    "Let's visualize how the agent learned over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(metrics[\"episode_rewards\"])\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# Plot moving average\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(metrics[\"avg_rewards\"])\n",
    "plt.title(\"Average Reward (last 100 episodes)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trained Agent\n",
    "\n",
    "Let's watch how the trained agent performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode\n",
    "state = await env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = await agent.act(state, training=False)\n",
    "    state, reward, done, info = await env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"\\nTest episode completed with total reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Agent\n",
    "\n",
    "Finally, let's save the trained agent for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"dqn_gridworld.pt\")\n",
    "print(\"Agent saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
